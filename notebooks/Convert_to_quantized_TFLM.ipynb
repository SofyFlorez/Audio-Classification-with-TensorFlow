{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d15d9dfe",
   "metadata": {},
   "source": [
    "# Convert the Keras h5 model to a quantized TFLite model\n",
    "\n",
    "This Jupyter notebook demonstrates the steps to take a pretrained Keras model and convert it to a quantized TensorFlow Lite (TFLite) model for optimized size and latency on mobile devices. Quantization refers to using lower precision numerical formats in the model calculations to reduce model size with minimal accuracy drop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6a6f415",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65b51f7",
   "metadata": {},
   "source": [
    "### Load Keras Model\n",
    "\n",
    "This code loads the previously trained Keras model to be converted. It will start with the full precision floating point Keras model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a70663d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the previously trained and saved Keras model\n",
    "model = tf.keras.models.load_model('/models/best_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f78b0a9",
   "metadata": {},
   "source": [
    "### Create TFLite Converter\n",
    "\n",
    "It uses a TFLiteConverter to convert the Keras model to TFLite format. The initial output is a floating point TFLite model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78adc7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a TFLite converter object from the Keras model\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c4a31c",
   "metadata": {},
   "source": [
    "### Set Optimization\n",
    "\n",
    "It sets the optimization level to use the TFLite default optimizations. This includes optimizations like fusion that will help optimize model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cfb742c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the optimization to use the default optimizations\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818df4c4",
   "metadata": {},
   "source": [
    "### Convert Model to Quantized TFLite\n",
    "\n",
    "This converts the model to a quantized TFLite model using full integer quantization for highest model compression. After quantization the model calculations use 8-bit integers instead of 32-bit floats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "256d08d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/pg/k661lbqs4sl4jplc4nftcthr0000gn/T/tmp3nkqhgm_/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/pg/k661lbqs4sl4jplc4nftcthr0000gn/T/tmp3nkqhgm_/assets\n",
      "2024-02-08 16:27:36.977904: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:378] Ignored output_format.\n",
      "2024-02-08 16:27:36.977919: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:381] Ignored drop_control_dependency.\n",
      "2024-02-08 16:27:36.978525: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /var/folders/pg/k661lbqs4sl4jplc4nftcthr0000gn/T/tmp3nkqhgm_\n",
      "2024-02-08 16:27:36.979743: I tensorflow/cc/saved_model/reader.cc:51] Reading meta graph with tags { serve }\n",
      "2024-02-08 16:27:36.979749: I tensorflow/cc/saved_model/reader.cc:146] Reading SavedModel debug info (if present) from: /var/folders/pg/k661lbqs4sl4jplc4nftcthr0000gn/T/tmp3nkqhgm_\n",
      "2024-02-08 16:27:36.981767: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:388] MLIR V1 optimization pass is not enabled\n",
      "2024-02-08 16:27:36.982731: I tensorflow/cc/saved_model/loader.cc:233] Restoring SavedModel bundle.\n",
      "2024-02-08 16:27:37.061425: I tensorflow/cc/saved_model/loader.cc:217] Running initialization op on SavedModel bundle at path: /var/folders/pg/k661lbqs4sl4jplc4nftcthr0000gn/T/tmp3nkqhgm_\n",
      "2024-02-08 16:27:37.071300: I tensorflow/cc/saved_model/loader.cc:316] SavedModel load for tags { serve }; Status: success: OK. Took 92776 microseconds.\n",
      "2024-02-08 16:27:37.084129: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "Summary on the non-converted ops:\n",
      "---------------------------------\n",
      " * Accepted dialects: tfl, builtin, func\n",
      " * Non-Converted Ops: 11, Total Ops 28, % non-converted = 39.29 %\n",
      " * 11 ARITH ops\n",
      "\n",
      "- arith.constant:   11 occurrences  (f32: 10, i32: 1)\n",
      "\n",
      "\n",
      "\n",
      "  (f32: 2)\n",
      "  (f32: 2)\n",
      "  (f32: 2)\n",
      "  (f32: 2)\n",
      "  (f32: 2)\n",
      "  (uq_8: 2)\n",
      "  (f32: 1)\n",
      "  (f32: 1)\n"
     ]
    }
   ],
   "source": [
    "# Convert the model to a quantized TFLite model\n",
    "tflite_quant_model = converter.convert()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244033b8",
   "metadata": {},
   "source": [
    "### Save Quantized Model\n",
    "\n",
    "The quantized TFLite model is saved to file for later use in applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32b99a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the quantized model to file \n",
    "with open('/models/quant_model.tflite', 'wb') as f:\n",
    "    f.write(tflite_quant_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5a2931",
   "metadata": {},
   "source": [
    "## Compare Model Sizes\n",
    "\n",
    "Finally, model sizes are printed out to show the compression achieved by quantization. The quantized model is almost 4x smaller than the floating point TFLite model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20f73bc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Integer quantization model saved!\n",
      "Initial model in Mb: 42.47442626953125\n",
      "Float model in Mb: 14.140071868896484\n",
      "Quantized model in Mb: 3.5412826538085938\n",
      "Compression ratio: 3.99292382201942\n"
     ]
    }
   ],
   "source": [
    "print(\"Full Integer quantization model saved!\")\n",
    "\n",
    "# Print model sizes for comparison\n",
    "print(\"Initial model in Mb:\", os.path.getsize('/models/best_model.h5') / float(2**20))\n",
    "print(\"Float model in Mb:\", os.path.getsize('/models/model.tflite') / float(2**20))\n",
    "print(\"Quantized model in Mb:\", os.path.getsize('/models/quant_model.tflite') / float(2**20))\n",
    "\n",
    "# Print compression ratio between float and quantized model\n",
    "print(\"Compression ratio:\", os.path.getsize('/models/model.tflite')/os.path.getsize('/models/quant_model.tflite'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
