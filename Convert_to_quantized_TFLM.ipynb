{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6a6f415",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Code to convert the keras h5 model to a quantized TFLite model\n",
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a70663d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the previously trained and saved Keras model\n",
    "model = tf.keras.models.load_model('best_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78adc7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a TFLite converter object from the Keras model\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cfb742c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the optimization to use the default optimizations\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "256d08d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/pg/k661lbqs4sl4jplc4nftcthr0000gn/T/tmp3nkqhgm_/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/pg/k661lbqs4sl4jplc4nftcthr0000gn/T/tmp3nkqhgm_/assets\n",
      "2024-02-08 16:27:36.977904: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:378] Ignored output_format.\n",
      "2024-02-08 16:27:36.977919: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:381] Ignored drop_control_dependency.\n",
      "2024-02-08 16:27:36.978525: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /var/folders/pg/k661lbqs4sl4jplc4nftcthr0000gn/T/tmp3nkqhgm_\n",
      "2024-02-08 16:27:36.979743: I tensorflow/cc/saved_model/reader.cc:51] Reading meta graph with tags { serve }\n",
      "2024-02-08 16:27:36.979749: I tensorflow/cc/saved_model/reader.cc:146] Reading SavedModel debug info (if present) from: /var/folders/pg/k661lbqs4sl4jplc4nftcthr0000gn/T/tmp3nkqhgm_\n",
      "2024-02-08 16:27:36.981767: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:388] MLIR V1 optimization pass is not enabled\n",
      "2024-02-08 16:27:36.982731: I tensorflow/cc/saved_model/loader.cc:233] Restoring SavedModel bundle.\n",
      "2024-02-08 16:27:37.061425: I tensorflow/cc/saved_model/loader.cc:217] Running initialization op on SavedModel bundle at path: /var/folders/pg/k661lbqs4sl4jplc4nftcthr0000gn/T/tmp3nkqhgm_\n",
      "2024-02-08 16:27:37.071300: I tensorflow/cc/saved_model/loader.cc:316] SavedModel load for tags { serve }; Status: success: OK. Took 92776 microseconds.\n",
      "2024-02-08 16:27:37.084129: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "Summary on the non-converted ops:\n",
      "---------------------------------\n",
      " * Accepted dialects: tfl, builtin, func\n",
      " * Non-Converted Ops: 11, Total Ops 28, % non-converted = 39.29 %\n",
      " * 11 ARITH ops\n",
      "\n",
      "- arith.constant:   11 occurrences  (f32: 10, i32: 1)\n",
      "\n",
      "\n",
      "\n",
      "  (f32: 2)\n",
      "  (f32: 2)\n",
      "  (f32: 2)\n",
      "  (f32: 2)\n",
      "  (f32: 2)\n",
      "  (uq_8: 2)\n",
      "  (f32: 1)\n",
      "  (f32: 1)\n"
     ]
    }
   ],
   "source": [
    "# Convert the model to a quantized TFLite model\n",
    "tflite_quant_model = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32b99a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the quantized model to file \n",
    "with open('quant_model.tflite', 'wb') as f:\n",
    "    f.write(tflite_quant_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20f73bc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Integer quantization model saved!\n",
      "Initial model in Mb: 42.47442626953125\n",
      "Float model in Mb: 14.140071868896484\n",
      "Quantized model in Mb: 3.5412826538085938\n",
      "Compression ratio: 3.99292382201942\n"
     ]
    }
   ],
   "source": [
    "print(\"Full Integer quantization model saved!\")\n",
    "\n",
    "# Print model sizes for comparison\n",
    "print(\"Initial model in Mb:\", os.path.getsize('best_model.h5') / float(2**20))\n",
    "print(\"Float model in Mb:\", os.path.getsize('model.tflite') / float(2**20))\n",
    "print(\"Quantized model in Mb:\", os.path.getsize('quant_model.tflite') / float(2**20))\n",
    "\n",
    "# Print compression ratio between float and quantized model\n",
    "print(\"Compression ratio:\", os.path.getsize('model.tflite')/os.path.getsize('quant_model.tflite'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
